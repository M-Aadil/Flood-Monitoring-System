# Flood-Monitoring-System
Built a deep learning model to automate the detection of flood events using satellite imagery. This workflow can be applied to lower the cost, improve the efficiency, and significantly enhance the effectiveness of various natural disaster management use cases.

This system consists of the below workflow:

1. Satellite remote sensors capture data
2. Data are used to (continuously) train deep learning neural network models
3. Different models and versions are managed by the model repository
4. Model inference performance is actively monitored
5. Data are passed to the inference server
6. The deep learning inference results are post-processed for either
7. Further analytics by 3rd party or
8. Raising alerts
<br>   

![Img](https://github.com/user-attachments/assets/955f72f2-b605-408c-b387-cd79c72cf45b)
<br>
<br>
<br>
<br>
Deep Learning Model Training Workflow: Building a deep learning model consists of several steps, including collecting large, high-quality 
datasets, preparing the data, training the model, and optimizing the model for deployment.

![Img 2](https://github.com/user-attachments/assets/7026f55b-697c-4c6e-b76b-a0a482fba17c)
<br>
<br>
<br>
<br>
Deep Learning Challenges: These are some challenges related to developing deep learning-based solutions.

1. Training accurate deep learning models from scratch requires a large amount of data and acquiring them is a costly process since they need to be annotated, often manually.
2. Development often requires knowledge of one or more deep learning frameworks, such as TensorFlow, PyTorch, or Caffe.
3. Deep learning models require significant effort to fine-tune before it is optimized for inference and production ready.
4. Processing data in real-time is computationally intensive and needs to be facilitated by software and hardware that enables low latency and high throughput.




